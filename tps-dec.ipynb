{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Introduction: Tabular Playground Series - Dec 2021\n> The objective of this notebook is to apply step-by-step approach to solve a tabular data competition on Kaggle.\n> \n> The subject of this notebook is [a multi-classification task](https://www.kaggle.com/c/tabular-playground-series-dec-2021/data)\n> \n> The target variable we are predicting consists of 7 different types of forest cover.\n>\n> The training dataset consists of 4 million labeled samples with features like elevation, soil type, etc.\n>\n> The provided dataset was synthetically generated by a GAN that was trained on a the data from the [Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction/overview). This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.\n> \n> Please refer to this [data page](https://www.kaggle.com/c/forest-cover-type-prediction/data) for a detailed explanation of the features.\n>\n> For the purposes of this notebook I will refer to held back training data as validation data, and data we have to submit predictions on as test data","metadata":{}},{"cell_type":"markdown","source":"## Changelog\n\n* Version 1 - basic eda and linear model -> 0.209 accuracy on validation dataset\n* Version 2 - established null accuracy baseline -> 0.565 accuracy on train dataset\n* Version 3 - added train-test split, standard scaler, SGD Classifier -> 0.8808 accuracy on public leaderboard (a fraction of the test dataset)\n* Version 3.1 - added Linear SVC -> 0.8805 accuracy on public leaderboard\n* Version 4 - added XGBoost -> 0.91796 accuracy on public leaderboard\n* Version 4.1 - added CatBoost -> 0.94155 accuracy on public leaderboard\n* Version 4.2 - added LightGBM -> 0.92976 accuracy on public leaderboard\n* Version 4.3 - dropped class 5 with only 1 occurance out of 4 million, updated Catboost -> 0.94135 accuracy on public leaderboard\n* Version 5 - add stratified K fold cross validation","metadata":{}},{"cell_type":"markdown","source":"## Import","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-09T23:05:33.143148Z","iopub.execute_input":"2021-12-09T23:05:33.143474Z","iopub.status.idle":"2021-12-09T23:05:33.159049Z","shell.execute_reply.started":"2021-12-09T23:05:33.143442Z","shell.execute_reply":"2021-12-09T23:05:33.158046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Read datasets to pandas dataframe\ndf_train = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/test.csv')\ndf_sample_submission = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T23:05:38.103619Z","iopub.execute_input":"2021-12-09T23:05:38.103985Z","iopub.status.idle":"2021-12-09T23:05:54.889521Z","shell.execute_reply.started":"2021-12-09T23:05:38.103936Z","shell.execute_reply":"2021-12-09T23:05:54.888024Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Reduce Memory Usage\n\nI have used a compression function by Guillaume Martin which is discussed here: https://www.kaggle.com/c/tabular-playground-series-dec-2021/discussion/291844\n","metadata":{}},{"cell_type":"code","source":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() / 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() / 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) / start_mem))\n    return df","metadata":{"execution":{"iopub.status.busy":"2021-12-09T23:05:54.891832Z","iopub.execute_input":"2021-12-09T23:05:54.892183Z","iopub.status.idle":"2021-12-09T23:05:54.90829Z","shell.execute_reply.started":"2021-12-09T23:05:54.892139Z","shell.execute_reply":"2021-12-09T23:05:54.906805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train = reduce_mem_usage(df_train)\ndf_test = reduce_mem_usage(df_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T23:05:56.476829Z","iopub.execute_input":"2021-12-09T23:05:56.477125Z","iopub.status.idle":"2021-12-09T23:06:18.962007Z","shell.execute_reply.started":"2021-12-09T23:05:56.477093Z","shell.execute_reply":"2021-12-09T23:06:18.96097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## EDA","metadata":{}},{"cell_type":"code","source":"# Checking out df_train\ndf_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T22:54:10.735486Z","iopub.execute_input":"2021-12-08T22:54:10.735811Z","iopub.status.idle":"2021-12-08T22:54:15.705904Z","shell.execute_reply.started":"2021-12-08T22:54:10.735764Z","shell.execute_reply":"2021-12-08T22:54:15.704882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see if we have any missing values\nmissing_values_train = df_train.isna().any().sum()\nmissing_values_test = df_test.isna().any().sum()\nprint(f'There are {missing_values_train} missing values in the train dataset')\nprint(f'There are {missing_values_test} missing values in the test dataset')","metadata":{"execution":{"iopub.status.busy":"2021-12-09T23:06:38.174472Z","iopub.execute_input":"2021-12-09T23:06:38.174759Z","iopub.status.idle":"2021-12-09T23:06:38.274152Z","shell.execute_reply.started":"2021-12-09T23:06:38.174727Z","shell.execute_reply":"2021-12-09T23:06:38.273124Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# What are the datatypes for our features?\nfor col in df_train:\n    print(df_train[col].dtype, col)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T22:54:46.980017Z","iopub.execute_input":"2021-12-08T22:54:46.980294Z","iopub.status.idle":"2021-12-08T22:54:47.010851Z","shell.execute_reply.started":"2021-12-08T22:54:46.980263Z","shell.execute_reply":"2021-12-08T22:54:47.00951Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets see which features are the most correlated with target\ndf_train.corr()['Cover_Type'].sort_values()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T23:06:44.458191Z","iopub.execute_input":"2021-12-09T23:06:44.458834Z","iopub.status.idle":"2021-12-09T23:07:16.696121Z","shell.execute_reply.started":"2021-12-09T23:06:44.458799Z","shell.execute_reply":"2021-12-09T23:07:16.695128Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Lets establish a baseline if we just always predict the target's most common class\n# AKA: null accuracy\ndf_train['Cover_Type'].value_counts(normalize=True).head(1)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T23:07:26.05258Z","iopub.execute_input":"2021-12-09T23:07:26.052879Z","iopub.status.idle":"2021-12-09T23:07:26.083022Z","shell.execute_reply.started":"2021-12-09T23:07:26.052847Z","shell.execute_reply":"2021-12-09T23:07:26.082072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since the accuracy for a model that only predicts class 2 would be 56.5%, we can judge the models we create by how much they can beat this 'dumb model'","metadata":{}},{"cell_type":"code","source":"# How imbalanced are the class distrubutions in our target variable?\ndf_train.groupby('Cover_Type').size()","metadata":{"execution":{"iopub.status.busy":"2021-12-09T23:07:46.991701Z","iopub.execute_input":"2021-12-09T23:07:46.992035Z","iopub.status.idle":"2021-12-09T23:07:47.069787Z","shell.execute_reply.started":"2021-12-09T23:07:46.991974Z","shell.execute_reply":"2021-12-09T23:07:47.068684Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Since there is only 1 occurrence of class 5 and there are only 377 occurrences of class 4 (out of 4 million samples in the train dataset) we could arguably drop both, for now lets just drop class 5","metadata":{}},{"cell_type":"code","source":"df_train = df_train[df_train['Cover_Type']!=5]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T23:07:52.634917Z","iopub.execute_input":"2021-12-09T23:07:52.635235Z","iopub.status.idle":"2021-12-09T23:07:53.108157Z","shell.execute_reply.started":"2021-12-09T23:07:52.635177Z","shell.execute_reply":"2021-12-09T23:07:53.107223Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"If the dataset hadn't already converted categorical features into dummy variables, we would do that here","metadata":{}},{"cell_type":"code","source":"# Create list of features without'id' and target variable 'cover_type'\nfeatures = list(df_train.columns)\nfeatures = features[1:55]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:45:53.350477Z","iopub.execute_input":"2021-12-09T21:45:53.350773Z","iopub.status.idle":"2021-12-09T21:45:53.35617Z","shell.execute_reply.started":"2021-12-09T21:45:53.350741Z","shell.execute_reply":"2021-12-09T21:45:53.355257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create feature dataframe and target dataframe for training\nX = df_train[features]\nY = df_train[\"Cover_Type\"]\n# Also create feature dataframe to generate our prediction\nX_test = df_test[features]","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:45:55.265706Z","iopub.execute_input":"2021-12-09T21:45:55.266482Z","iopub.status.idle":"2021-12-09T21:45:56.124019Z","shell.execute_reply.started":"2021-12-09T21:45:55.266447Z","shell.execute_reply":"2021-12-09T21:45:56.12303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Do the train test split before standardizing our features (to prevent data leak)\n# Since the dataset is large we could do a smaller test_size than .2,\n# Even better would be to implement StratifiedKFold, ie 5 folds of .2 with class imbalance replicated in each fold\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_validate, Y_train, Y_validate = train_test_split( X, Y, test_size=0.2, random_state=2)\nprint ('Train set:', X_train.shape,  Y_train.shape)\nprint ('Validation set:', X_validate.shape,  Y_validate.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:45:58.261097Z","iopub.execute_input":"2021-12-09T21:45:58.261471Z","iopub.status.idle":"2021-12-09T21:46:01.12497Z","shell.execute_reply.started":"2021-12-09T21:45:58.261426Z","shell.execute_reply":"2021-12-09T21:46:01.124038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Implement StratifiedKFold","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_validate = scaler.transform (X_validate)\nX_test = scaler.fit_transform(X_test)\n\ndel df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:46:01.127132Z","iopub.execute_input":"2021-12-09T21:46:01.127476Z","iopub.status.idle":"2021-12-09T21:46:07.79396Z","shell.execute_reply.started":"2021-12-09T21:46:01.127431Z","shell.execute_reply":"2021-12-09T21:46:07.793015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Modeling","metadata":{}},{"cell_type":"markdown","source":"We are predicting a category, have labled data, and >100K samples\n\ndone-ish:\n* SGD\n* Linear SVC\n* XGBoost\n* CatBoost\n* Light GBM\n\nmay add:\n* Random Forest\n* KNeighbors Classifier\n* SVC","metadata":{}},{"cell_type":"markdown","source":"### SGD Classifier (stochastic gradient descent)\n\nSGD classifier allows you to select a loss function, we will use the default, which is equivalent to a Linear SVM (but faster)","metadata":{}},{"cell_type":"code","source":"# Create SGD model\nfrom sklearn.linear_model import SGDClassifier\nsgdmodel = SGDClassifier(loss='hinge',  penalty='l2')\nsgdmodel.fit(X_train,Y_train)\n# R^2 for training data\nsgdmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T22:54:58.906122Z","iopub.execute_input":"2021-12-08T22:54:58.906349Z","iopub.status.idle":"2021-12-08T22:56:18.740095Z","shell.execute_reply.started":"2021-12-08T22:54:58.90632Z","shell.execute_reply":"2021-12-08T22:56:18.739097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\nsgdmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T22:56:18.741274Z","iopub.execute_input":"2021-12-08T22:56:18.741495Z","iopub.status.idle":"2021-12-08T22:56:19.017045Z","shell.execute_reply.started":"2021-12-08T22:56:18.741464Z","shell.execute_reply":"2021-12-08T22:56:19.016097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create test data prediction\n# sgdmodel.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T22:56:19.018343Z","iopub.execute_input":"2021-12-08T22:56:19.018577Z","iopub.status.idle":"2021-12-08T22:56:19.022738Z","shell.execute_reply.started":"2021-12-08T22:56:19.018547Z","shell.execute_reply":"2021-12-08T22:56:19.021763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Linear SVC","metadata":{}},{"cell_type":"code","source":"# Create Linear SVC model\nfrom sklearn.svm import LinearSVC\nlsvcmodel = LinearSVC(penalty='l2', loss='squared_hinge')\nlsvcmodel.fit(X_train,Y_train)\n# R^2 for training data\nlsvcmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T23:03:55.050774Z","iopub.execute_input":"2021-12-08T23:03:55.051111Z","iopub.status.idle":"2021-12-09T01:33:39.615841Z","shell.execute_reply.started":"2021-12-08T23:03:55.051076Z","shell.execute_reply":"2021-12-09T01:33:39.614211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\nlsvcmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:33:39.620236Z","iopub.execute_input":"2021-12-09T01:33:39.620563Z","iopub.status.idle":"2021-12-09T01:33:39.989001Z","shell.execute_reply.started":"2021-12-09T01:33:39.620528Z","shell.execute_reply":"2021-12-09T01:33:39.987994Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### XGBoost","metadata":{}},{"cell_type":"markdown","source":"For this version the hyperparameters are arbitrary, for a future version we could do a grid search to establish the best performing hyperparameters, then we could fit the model again without GPU acceleration to improve accuracy","metadata":{}},{"cell_type":"code","source":"# Create XGBoost model\nfrom xgboost import XGBClassifier # Alternatively there is a sklearn wrapper, from sklearn.ensemble import GradientBoostingClassifier\n\nparams = {\n#             'objective':'binary:logistic',/\n            'objective' : 'multi:softmax',\n            'tree_method': 'gpu_hist',\n            'eval_metric': 'mlogloss',\n            'booster' : 'gbtree',\n            'gamma' : 0.75,\n            'max_depth': 7,\n            'alpha': 10,\n            'learning_rate': .007,\n            'n_estimators':2000,\n            'predictor': 'gpu_predictor'\n        }  \n\nxgbmodel = XGBClassifier(**params)\n\nxgbmodel.fit(X_train,Y_train,\n               early_stopping_rounds=200,\n               eval_set=[(X_validate,Y_validate)],\n               verbose=True)\n\n# R^2 for training data\nxgbmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:32:06.251416Z","iopub.execute_input":"2021-12-09T20:32:06.25165Z","iopub.status.idle":"2021-12-09T20:37:51.571397Z","shell.execute_reply.started":"2021-12-09T20:32:06.251619Z","shell.execute_reply":"2021-12-09T20:37:51.570692Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\nxgbmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:38:11.209106Z","iopub.execute_input":"2021-12-09T20:38:11.209487Z","iopub.status.idle":"2021-12-09T20:38:17.726322Z","shell.execute_reply.started":"2021-12-09T20:38:11.209444Z","shell.execute_reply":"2021-12-09T20:38:17.725644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CatBoost","metadata":{}},{"cell_type":"code","source":"# Create CatBoost model\nfrom catboost import CatBoostClassifier\ncatbmodel = CatBoostClassifier(task_type = 'GPU', devices='0')\ncatbmodel.fit(X_train, Y_train)\n\n# R^2 for training data\ncatbmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:49:01.836352Z","iopub.execute_input":"2021-12-09T20:49:01.836927Z","iopub.status.idle":"2021-12-09T20:51:14.46131Z","shell.execute_reply.started":"2021-12-09T20:49:01.836885Z","shell.execute_reply":"2021-12-09T20:51:14.460614Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\ncatbmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:51:14.462999Z","iopub.execute_input":"2021-12-09T20:51:14.463401Z","iopub.status.idle":"2021-12-09T20:51:19.060612Z","shell.execute_reply.started":"2021-12-09T20:51:14.463365Z","shell.execute_reply":"2021-12-09T20:51:19.059984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LGBM","metadata":{}},{"cell_type":"code","source":"# Create LightGBM model\nfrom lightgbm import LGBMClassifier\n\nlgb_params = {\n    'objective' : 'multiclass',\n    'metric' : 'multi_logloss',\n    'device' : 'gpu',\n}\n\nlgbmmodel = LGBMClassifier(**lgb_params)\n\nlgbmmodel.fit(X_train,Y_train,\n               early_stopping_rounds=200,\n               eval_set=[(X_validate,Y_validate)],\n               verbose=True)\n\n# R^2 for training data\nlgbmmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:46:13.893893Z","iopub.execute_input":"2021-12-09T21:46:13.894782Z","iopub.status.idle":"2021-12-09T21:49:40.883827Z","shell.execute_reply.started":"2021-12-09T21:46:13.894748Z","shell.execute_reply":"2021-12-09T21:49:40.882985Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\nlgbmmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:49:40.888954Z","iopub.execute_input":"2021-12-09T21:49:40.891401Z","iopub.status.idle":"2021-12-09T21:49:46.100231Z","shell.execute_reply.started":"2021-12-09T21:49:40.891355Z","shell.execute_reply":"2021-12-09T21:49:46.099262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Prepare Submission","metadata":{}},{"cell_type":"code","source":"# View sample submission\ndf_sample_submission","metadata":{"execution":{"iopub.status.busy":"2021-12-08T22:56:19.099395Z","iopub.execute_input":"2021-12-08T22:56:19.09999Z","iopub.status.idle":"2021-12-08T22:56:19.119644Z","shell.execute_reply.started":"2021-12-08T22:56:19.099956Z","shell.execute_reply":"2021-12-08T22:56:19.118806Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"sgdmodel (public score = 0.88080)","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_sgd_submission = df_sample_submission\ndf_sgd_submission['Cover_Type'] = sgdmodel.predict(X_test).astype('int')\ndf_sgd_submission.to_csv(\"sgd_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T22:56:19.121017Z","iopub.execute_input":"2021-12-08T22:56:19.121766Z","iopub.status.idle":"2021-12-08T22:56:21.243189Z","shell.execute_reply.started":"2021-12-08T22:56:19.121729Z","shell.execute_reply":"2021-12-08T22:56:21.242076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"xgbmodel (public score = 0.91796)\n","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_xgb_submission = df_sample_submission\ndf_xgb_submission['Cover_Type'] = xgbmodel.predict(X_test).astype('int')\ndf_xgb_submission.to_csv(\"xgb_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:39:35.354646Z","iopub.execute_input":"2021-12-09T20:39:35.355345Z","iopub.status.idle":"2021-12-09T20:39:45.594488Z","shell.execute_reply.started":"2021-12-09T20:39:35.355295Z","shell.execute_reply":"2021-12-09T20:39:45.593772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lsvcmodel (public score = 0.88050)","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_lsvc_submission = df_sample_submission\ndf_lsvc_submission['Cover_Type'] = lsvcmodel.predict(X_test).astype('int')\ndf_lsvc_submission.to_csv(\"lsvc_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T01:33:39.990319Z","iopub.execute_input":"2021-12-09T01:33:39.990645Z","iopub.status.idle":"2021-12-09T01:33:42.764241Z","shell.execute_reply.started":"2021-12-09T01:33:39.990611Z","shell.execute_reply":"2021-12-09T01:33:42.763276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"catbmodel (public score = 0.94155)","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_catb_submission = df_sample_submission\ndf_catb_submission['Cover_Type'] = catbmodel.predict(X_test).astype('int')\ndf_catb_submission.to_csv(\"catb_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T20:51:19.061832Z","iopub.execute_input":"2021-12-09T20:51:19.062086Z","iopub.status.idle":"2021-12-09T20:51:27.32999Z","shell.execute_reply.started":"2021-12-09T20:51:19.062051Z","shell.execute_reply":"2021-12-09T20:51:27.329239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lgbmmodel (public score = 0.92976)","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_lgbm_submission = df_sample_submission\ndf_lgbm_submission['Cover_Type'] = lgbmmodel.predict(X_test).astype('int')\ndf_lgbm_submission.to_csv(\"lgbm_submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2021-12-09T21:50:33.778597Z","iopub.execute_input":"2021-12-09T21:50:33.778904Z","iopub.status.idle":"2021-12-09T21:50:42.141185Z","shell.execute_reply.started":"2021-12-09T21:50:33.778873Z","shell.execute_reply":"2021-12-09T21:50:42.140151Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
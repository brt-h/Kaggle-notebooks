{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Tabular Playground Series - Dec 2021\n> The objective of this notebook is to apply step-by-step approach to solve a tabular data competition on Kaggle.\n> \n> The subject of this notebook is [a multi-classification task](https://www.kaggle.com/c/tabular-playground-series-dec-2021/data). \n> \n> The provided dataset was synthetically generated by a GAN that was trained on a the data from the [Forest Cover Type Prediction](https://www.kaggle.com/c/forest-cover-type-prediction/overview). This dataset is (a) much larger, and (b) may or may not have the same relationship to the target as the original data.\n> \n> Please refer to this [data page](https://www.kaggle.com/c/forest-cover-type-prediction/data) for a detailed explanation of the features.","metadata":{}},{"cell_type":"markdown","source":"## EXAMPLE Table of Contents\n1. Import\n1. EDA\n1. Relationship with numerical variables\n1. Correlation with heatmap\n1. Missing Value (data preprocessing)\n1. Divided categorical and Numerical\n1. Encoder features\n1. Normalization\n1. Convert Into Numpy array\n1. Classifier\n    * XGBOOST\n    * CatBOOST\n    \n    \n## todo:\n### - import ml libraries\n### - check for missing values\n### - train test split the train dataset to validate hyperparameter tuning\n### - standardize + \n### - train on train data\n### - evaluate fit with test data","metadata":{}},{"cell_type":"markdown","source":"## Step 1. Import","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-12-08T17:58:07.156524Z","iopub.execute_input":"2021-12-08T17:58:07.156846Z","iopub.status.idle":"2021-12-08T17:58:07.175373Z","shell.execute_reply.started":"2021-12-08T17:58:07.156815Z","shell.execute_reply":"2021-12-08T17:58:07.174386Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Read datasets to pandas dataframe\ndf_train = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/train.csv')\ndf_test = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/test.csv')\ndf_sample_submission = pd.read_csv('/kaggle/input/tabular-playground-series-dec-2021/sample_submission.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:58:07.177262Z","iopub.execute_input":"2021-12-08T17:58:07.178853Z","iopub.status.idle":"2021-12-08T17:58:32.046106Z","shell.execute_reply.started":"2021-12-08T17:58:07.178804Z","shell.execute_reply":"2021-12-08T17:58:32.045176Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Step 2. EDA","metadata":{}},{"cell_type":"code","source":"# Checking out df_train\ndf_train.describe()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:58:32.047538Z","iopub.execute_input":"2021-12-08T17:58:32.047786Z","iopub.status.idle":"2021-12-08T17:58:37.462589Z","shell.execute_reply.started":"2021-12-08T17:58:32.047757Z","shell.execute_reply":"2021-12-08T17:58:37.461730Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Lets see if we have any missing values\nmissing_values_train = df_train.isna().any().sum()\nmissing_values_test = df_test.isna().any().sum()\nprint(f'There are {missing_values_train} missing values in the train dataset')\nprint(f'There are {missing_values_test} missing values in the test dataset')","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-12-08T17:58:37.465086Z","iopub.execute_input":"2021-12-08T17:58:37.465454Z","iopub.status.idle":"2021-12-08T17:58:37.540362Z","shell.execute_reply.started":"2021-12-08T17:58:37.465425Z","shell.execute_reply":"2021-12-08T17:58:37.539378Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# How imbalanced are the class distrubutions in our target variable?\ndf_train.groupby('Cover_Type').size()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:58:37.541842Z","iopub.execute_input":"2021-12-08T17:58:37.542162Z","iopub.status.idle":"2021-12-08T17:58:37.616221Z","shell.execute_reply.started":"2021-12-08T17:58:37.542118Z","shell.execute_reply":"2021-12-08T17:58:37.615314Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Since there is only 1 occurrence of class 5 and there are only 377 occurrences of class 4 (out of 4 million samples in the train dataset) we could drop both without affecting our model's accuracy. For now we will leave them.","metadata":{}},{"cell_type":"code","source":"# Lets establish a baseline if we just always predict the target's most common class\n# AKA: null accuracy\ndf_train['Cover_Type'].value_counts(normalize=True).head(1)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:58:37.617460Z","iopub.execute_input":"2021-12-08T17:58:37.617709Z","iopub.status.idle":"2021-12-08T17:58:37.649840Z","shell.execute_reply.started":"2021-12-08T17:58:37.617658Z","shell.execute_reply":"2021-12-08T17:58:37.648794Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"Since the accuracy for a model that only predicts class 2 would be 56.5%, we can judge the models we create by how much they can beat this 'dumb model'","metadata":{}},{"cell_type":"code","source":"# Lets see which features are the most correlated with target\ndf_train.corr()['Cover_Type'].sort_values()","metadata":{"jupyter":{"source_hidden":true},"execution":{"iopub.status.busy":"2021-12-08T17:58:37.651262Z","iopub.execute_input":"2021-12-08T17:58:37.652148Z","iopub.status.idle":"2021-12-08T17:59:17.217302Z","shell.execute_reply.started":"2021-12-08T17:58:37.652110Z","shell.execute_reply":"2021-12-08T17:59:17.216430Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# What are the datatypes for our features?\nfor col in df_train:\n    print(df_train[col].dtype, col)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:59:17.218775Z","iopub.execute_input":"2021-12-08T17:59:17.219242Z","iopub.status.idle":"2021-12-08T17:59:17.244739Z","shell.execute_reply.started":"2021-12-08T17:59:17.219196Z","shell.execute_reply":"2021-12-08T17:59:17.244067Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Step 3. Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"If the dataset hadn't already converted categorical features into dummy variables, we would do that here","metadata":{}},{"cell_type":"code","source":"# Create list of features without'id' and target variable 'cover_type'\nfeatures = list(df_train.columns)\nfeatures = features[1:55]","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:59:17.246178Z","iopub.execute_input":"2021-12-08T17:59:17.246383Z","iopub.status.idle":"2021-12-08T17:59:17.250581Z","shell.execute_reply.started":"2021-12-08T17:59:17.246358Z","shell.execute_reply":"2021-12-08T17:59:17.249853Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Create feature dataframe and target dataframe for training\nX = df_train[features]\nY = df_train[\"Cover_Type\"]\n# Also create feature dataframe to generate our prediction\nX_test = df_test[features]","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:59:17.253724Z","iopub.execute_input":"2021-12-08T17:59:17.254242Z","iopub.status.idle":"2021-12-08T17:59:18.255518Z","shell.execute_reply.started":"2021-12-08T17:59:17.254210Z","shell.execute_reply":"2021-12-08T17:59:18.254476Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Do the train test split before standardizing our features (to prevent data leak)\n# Since the dataset is large we could do a smaller test_size than .2,\n# Even better would be to implement cross validation, ie 5 folds of .2 \nfrom sklearn.model_selection import train_test_split\n\nX_train, X_validate, Y_train, Y_validate = train_test_split( X, Y, test_size=0.2, random_state=2)\nprint ('Train set:', X_train.shape,  Y_train.shape)\nprint ('Validation set:', X_validate.shape,  Y_validate.shape)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:59:18.256911Z","iopub.execute_input":"2021-12-08T17:59:18.257145Z","iopub.status.idle":"2021-12-08T17:59:23.569113Z","shell.execute_reply.started":"2021-12-08T17:59:18.257116Z","shell.execute_reply":"2021-12-08T17:59:23.568391Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_validate = scaler.transform (X_validate)\nX_test = scaler.fit_transform(X_test)\n\ndel df_train, df_test","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:59:23.570479Z","iopub.execute_input":"2021-12-08T17:59:23.571274Z","iopub.status.idle":"2021-12-08T17:59:29.832506Z","shell.execute_reply.started":"2021-12-08T17:59:23.571234Z","shell.execute_reply":"2021-12-08T17:59:29.831810Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Step 4: Modeling","metadata":{}},{"cell_type":"markdown","source":"Since we are predicting a category, have labled data, and >100K samples I want to test the performance of:\n* SGD Classifier\n* kernel approximation\n\nI will also test the following estimators that are better with <100K samples:\n* Linear SVC\n* KNeighbors Classifier\n* SVC\n\nAlso I totally forgot about the new hype:\n* xgboost","metadata":{}},{"cell_type":"markdown","source":"### Step 4.1: SGD Classifier (stochastic gradient descent)\n\nSGD classifier allows you to select a loss function, we will use the default, which is equivalent to a Linear SVM (but faster)","metadata":{}},{"cell_type":"code","source":"# Create SGD model\nfrom sklearn.linear_model import SGDClassifier\nsgdmodel = SGDClassifier(loss='hinge',  penalty='l2')\nsgdmodel.fit(X_train,Y_train)\n# R^2 for training data\nsgdmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T17:59:29.834953Z","iopub.execute_input":"2021-12-08T17:59:29.835290Z","iopub.status.idle":"2021-12-08T18:00:48.650898Z","shell.execute_reply.started":"2021-12-08T17:59:29.835246Z","shell.execute_reply":"2021-12-08T18:00:48.650052Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\nsgdmodel.score(X_validate,Y_validate)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:00:48.652476Z","iopub.execute_input":"2021-12-08T18:00:48.653038Z","iopub.status.idle":"2021-12-08T18:00:48.894346Z","shell.execute_reply.started":"2021-12-08T18:00:48.652996Z","shell.execute_reply":"2021-12-08T18:00:48.893329Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Create test data prediction\n# sgdmodel.predict(X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:00:48.895914Z","iopub.execute_input":"2021-12-08T18:00:48.896490Z","iopub.status.idle":"2021-12-08T18:00:48.900520Z","shell.execute_reply.started":"2021-12-08T18:00:48.896437Z","shell.execute_reply":"2021-12-08T18:00:48.899518Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"### Step 4.2: Kernel Approximation","metadata":{}},{"cell_type":"code","source":"# ?","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:00:48.901979Z","iopub.execute_input":"2021-12-08T18:00:48.902197Z","iopub.status.idle":"2021-12-08T18:00:48.915749Z","shell.execute_reply.started":"2021-12-08T18:00:48.902169Z","shell.execute_reply":"2021-12-08T18:00:48.915063Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"### Step 4.3: Linear SVC","metadata":{}},{"cell_type":"code","source":"# class sklearn.svm.LinearSVC(penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:00:48.916745Z","iopub.execute_input":"2021-12-08T18:00:48.919407Z","iopub.status.idle":"2021-12-08T18:00:48.928686Z","shell.execute_reply.started":"2021-12-08T18:00:48.919359Z","shell.execute_reply":"2021-12-08T18:00:48.928085Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"# # Create Linear SVC model\n# from sklearn.svm import LinearSVC\n# lsvcmodel = LinearSVC(penalty='l2', loss='squared_hinge', *, dual=True, tol=0.0001, C=1.0, multi_class='ovr', fit_intercept=True, intercept_scaling=1, class_weight=None, verbose=0, random_state=None, max_iter=1000)\n# sgdmodel.fit(X_train,Y_train)\n# # R^2 for training data\n# sgdmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:00:48.929764Z","iopub.execute_input":"2021-12-08T18:00:48.930080Z","iopub.status.idle":"2021-12-08T18:00:48.942121Z","shell.execute_reply.started":"2021-12-08T18:00:48.930054Z","shell.execute_reply":"2021-12-08T18:00:48.941527Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"### Step 4.?: XGBoost","metadata":{}},{"cell_type":"code","source":"# # Create XGBoost model\n# from xgboost import XGBRegressor\n# xgbmodel = XGBRegressor()","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:00:48.943479Z","iopub.execute_input":"2021-12-08T18:00:48.943734Z","iopub.status.idle":"2021-12-08T18:00:48.960420Z","shell.execute_reply.started":"2021-12-08T18:00:48.943702Z","shell.execute_reply":"2021-12-08T18:00:48.959726Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\nxgbmodel = GradientBoostingClassifier()\nxgbmodel.fit(X_train,Y_train)\n# R^2 for training data\nxgbmodel.score(X_train,Y_train)","metadata":{"execution":{"iopub.status.busy":"2021-12-08T18:00:48.961977Z","iopub.execute_input":"2021-12-08T18:00:48.962393Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# R^2 for validation data\nxgbmodel.score(X_validate,Y_validate)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Step 5: Preparing Submission","metadata":{}},{"cell_type":"markdown","source":"sgdmodel","metadata":{}},{"cell_type":"code","source":"# View sample submission\ndf_sample_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_sgd_submission = df_sample_submission\ndf_sgd_submission['Cover_Type'] = sgdmodel.predict(X_test).astype('int')\ndf_sgd_submission.to_csv(\"sgd_submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sgd_submission","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"xgbmodel","metadata":{}},{"cell_type":"code","source":"# Rename df and replace the cover type column with our predictions\ndf_xgb_submission = df_sample_submission\ndf_xgb_submission['Cover_Type'] = xgbmodel.predict(X_test).astype('int')\ndf_xgb_submission.to_csv(\"xgb_submission.csv\",index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# kaggle competitions submit -c tabular-playground-series-dec-2021 -f submission.csv -m \"Message\"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}